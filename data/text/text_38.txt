3 Example: Variational Auto-Encoder
In this section we’ll give an example where we use a neural network for the probabilistic encoder qφ(z|x) (the approximation to the posterior of the generative model pθ(x,z)) and where the param- eters φ and θ are optimized jointly with the AEVB algorithm.
Let the prior over the latent variables be the centered isotropic multivariate Gaussian pθ(z) = N(z;0,I). Note that in this case, the prior lacks parameters. We let pθ(x|z) be a multivariate Gaussian (in case of real-valued data) or Bernoulli (in case of binary data) whose distribution pa- rameters are computed from z with a MLP (a fully-connected neural network with a single hidden layer, see appendix C). Note the true posterior pθ(z|x) is in this case intractable. While there is much freedom in the form qφ(z|x), we’ll assume the true (but intractable) posterior takes on a ap- proximate Gaussian form with an approximately diagonal covariance. In this case, we can let the variational approximate posterior be a multivariate Gaussian with a diagonal covariance structure2: