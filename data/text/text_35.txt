A connection with auto-encoders becomes clear when looking at the objective function given at eq. (7). The first term is (the KL divergence of the approximate posterior from the prior) acts as a regularizer, while the second term is a an expected negative reconstruction error. The function gg (.) is chosen such that it maps a datapoint x and a random noise vector €!) to a sample from the approximate posterior for that datapoint: 2) = gg(e,x) where 2) ~ qg(z|x). Subse- quently, the sample z“-!) is then input to function log pe(x|z)), which equals the probability density (or mass) of datapoint x under the generative model, given z‘*“"). This term is a negative reconstruction error in auto-encoder parlance.