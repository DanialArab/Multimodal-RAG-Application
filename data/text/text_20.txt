2.1 Problem scenario
Let us consider some dataset X = {x(i)}N i=1 consisting of N i.i.d. samples of some continuous or discrete variable x. We assume that the data are generated by some random process, involving an unobserved continuous random variable z. The process consists of two steps: (1) a value z(i) is generated from some prior distribution pθ∗(z); (2) a value x(i) is generated from some condi- tional distribution pθ∗(x|z). We assume that the prior pθ∗(z) and likelihood pθ∗(x|z) come from parametric families of distributions pθ(z) and pθ(x|z), and that their PDFs are differentiable almost everywhere w.r.t. both θ and z. Unfortunately, a lot of this process is hidden from our view: the true parameters θ∗ as well as the values of the latent variables z(i) are unknown to us.
Very importantly, we do not make the common simplifying assumptions about the marginal or pos- terior probabilities. Conversely, we are here interested in a general algorithm that even works efﬁ- ciently in the case of:
1. Intractability: the case where the integral of the marginal likelihood pe(x) = J po(z)pe(x|z) dz is intractable (so we cannot evaluate or differentiate the marginal like- lihood), where the true posterior density pg(z|x) = pe(x|z)pe(z)/pe(x) is intractable (so the EM algorithm cannot be used), and where the required integrals for any reason- able mean-field VB algorithm are also intractable. These intractabilities are quite common and appear in cases of moderately complicated likelihood functions pg (x|z), e.g. a neural network with a nonlinear hidden layer.
2. A large dataset: we have so much data that batch optimization is too costly; we would like to make parameter updates using small minibatches or even single datapoints. Sampling- based solutions, e.g. Monte Carlo EM, would in general be too slow, since it involves a typically expensive sampling loop per datapoint.
We are interested in, and propose a solution to, three related problems in the above scenario:
1. Efﬁcient approximate ML or MAP estimation for the parameters θ. The parameters can be of interest themselves, e.g. if we are analyzing some natural process. They also allow us to mimic the hidden random process and generate artiﬁcial data that resembles the real data.
2. Efﬁcient approximate posterior inference of the latent variable z given an observed value x for a choice of parameters θ. This is useful for coding or data representation tasks.
3. Efﬁcient approximate marginal inference of the variable x. This allows us to perform all kinds of inference tasks where a prior over x is required. Common applications in computer vision include image denoising, inpainting and super-resolution.