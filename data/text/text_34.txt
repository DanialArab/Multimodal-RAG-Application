where the minibatch KX! = {x}_ is a randomly drawn sample of M datapoints from the full dataset X with N datapoints. In our experiments we found that the number of samples L per datapoint can be set to 1 as long as the minibatch size M was large enough, e.g. M = 100. Derivatives Vo,¢£(0;X™) can be taken, and the resulting gradients can be used in conjunction with stochastic optimization methods such as SGD or Adagrad (DHS10]. See algorithm [I] for a basic approach to compute the stochastic gradients.