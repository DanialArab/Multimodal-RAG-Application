θ,φ ← Initialize parameters
0, & + Initialize parameters repeat X™ & Random minibatch of M datapoints (drawn from full dataset) e < Random samples from noise distribution p(e) g « VogLl™ (0,6; X™ ,€) (Gradients of minibatch estimator (8)) 0, @ < Update parameters using gradients g (e.g. SGD or Adagrad [DHS10]) until convergence of parameters (0, d) return 0,
Often, the KL-divergence Dx (qu(z|x)||pe(z)) of eq. GB) can be integrated analytically (see appendix By such that only the expected reconstruction error Ey (elx) [log po(x |z)| requires estimation by sampling. The KL-divergence term can then be interpreted as regularizing @, encour- aging the approximate posterior to be close to the prior pg(z). This yields a second version of the SGVB estimator £2 (0, o; x) ~ £(0, 6; x), corresponding to eq. (3), which typically has less variance than the generic estimator: